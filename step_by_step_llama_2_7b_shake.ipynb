{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOnyQ7u84FdUqetwDiLbIjd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4c4e64b1da14450d810c9d98195b05e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9f693d64bf54432c8d5f3fc568e5ec1b",
              "IPY_MODEL_aa226f39a5904a199948a689694c6b00",
              "IPY_MODEL_1646f15d3ccf46f2a7631398a525672c"
            ],
            "layout": "IPY_MODEL_b6eb4d5ce7f94f07a05d63fbc613668a"
          }
        },
        "9f693d64bf54432c8d5f3fc568e5ec1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e02bb7b6534d4cb8ac2e7e8b5c2b5928",
            "placeholder": "​",
            "style": "IPY_MODEL_81a477ace6ba4286a7467852d380d2bf",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "aa226f39a5904a199948a689694c6b00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4666886733384d6bab20e6d72bcfddd4",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ec65bbf93467420bb12aed49769bb5f8",
            "value": 3
          }
        },
        "1646f15d3ccf46f2a7631398a525672c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30700665d7194dd787e6e15b7f8a191e",
            "placeholder": "​",
            "style": "IPY_MODEL_abdd233845b64dd4b6300026a365490e",
            "value": " 3/3 [00:16&lt;00:00,  5.24s/it]"
          }
        },
        "b6eb4d5ce7f94f07a05d63fbc613668a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e02bb7b6534d4cb8ac2e7e8b5c2b5928": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81a477ace6ba4286a7467852d380d2bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4666886733384d6bab20e6d72bcfddd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec65bbf93467420bb12aed49769bb5f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "30700665d7194dd787e6e15b7f8a191e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abdd233845b64dd4b6300026a365490e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shake/colab-Llama-2-ipynb/blob/main/step_by_step_llama_2_7b_shake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sokFVxHpHxP9"
      },
      "outputs": [],
      "source": [
        "# huggingface cli\n",
        "!pip install -q huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import 密钥\n",
        "from google.colab import userdata\n",
        "hf_token = userdata.get('huggingface')\n",
        "!git config --global credential.helper store\n",
        "!huggingface-cli login --token $hf_token --add-to-git-credential"
      ],
      "metadata": {
        "id": "-XEFs7-YIvCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 安装微调需要包\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install sentencepiece\n",
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2  trl==0.4.7"
      ],
      "metadata": {
        "id": "pRZrbu0RJJXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 模型\n",
        "MODEL_ID = \"meta-llama/Llama-2-7b\"\n",
        "MODEL_NAME = MODEL_ID.split('/')[-1]"
      ],
      "metadata": {
        "id": "HqF1cuRGMdGI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download Llama-2-7b\n",
        "!huggingface-cli download \\\n",
        "\t--local-dir=/content/$MODEL_NAME \\\n",
        "\t$MODEL_ID \\\n",
        "\tchecklist.chk consolidated.00.pth params.json \\\n",
        "\ttokenizer.model tokenizer_checklist.chk"
      ],
      "metadata": {
        "id": "buPdSJvyI1Hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 下载HF格式转换工具\n",
        "!wget https://raw.githubusercontent.com/huggingface/transformers/main/src/transformers/models/llama/convert_llama_weights_to_hf.py"
      ],
      "metadata": {
        "id": "cLISvDB7Lv6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 格式转换需要提前创建好目录\n",
        "!mkdir /content/Llama-2-7b/7B\n",
        "!cp /content/Llama-2-7b/params.json /content/Llama-2-7b/7B/params.json\n"
      ],
      "metadata": {
        "id": "it6nN0wnKrb4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# update cache\n",
        "from transformers.utils.hub import move_cache"
      ],
      "metadata": {
        "id": "e71oQYAaQP5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#解决colab字符集错误\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "Mwv9jCAeS7zp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 开始转换\n",
        "!python convert_llama_weights_to_hf.py \\\n",
        "    --input_dir /content/$MODEL_NAME  --model_size 7B --output_dir $MODEL_NAME-hf"
      ],
      "metadata": {
        "id": "FlxDGL7oMATG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 查看转换结果\n",
        "!ls ./Llama-2-7b-hf"
      ],
      "metadata": {
        "id": "trjK0B0eVFJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#测试没有微调之前的模型\n",
        "import torch\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
        "model_id=\"/content/Llama-2-7b-hf\"\n",
        "tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
        "model =LlamaForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "4c4e64b1da14450d810c9d98195b05e0",
            "9f693d64bf54432c8d5f3fc568e5ec1b",
            "aa226f39a5904a199948a689694c6b00",
            "1646f15d3ccf46f2a7631398a525672c",
            "b6eb4d5ce7f94f07a05d63fbc613668a",
            "e02bb7b6534d4cb8ac2e7e8b5c2b5928",
            "81a477ace6ba4286a7467852d380d2bf",
            "4666886733384d6bab20e6d72bcfddd4",
            "ec65bbf93467420bb12aed49769bb5f8",
            "30700665d7194dd787e6e15b7f8a191e",
            "abdd233845b64dd4b6300026a365490e"
          ]
        },
        "id": "DDRjx-cuVslf",
        "outputId": "65f5f1bd-4964-4355-fa37-b47676975252"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c4e64b1da14450d810c9d98195b05e0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 测试\n",
        "eval_prompt = \"\"\"\n",
        "Summarize this dialog:\n",
        "A: Hi Tom, are you busy tomorrow’s afternoon?\n",
        "B: I’m pretty sure I am. What’s up?\n",
        "A: Can you go with me to the animal shelter?.\n",
        "B: What do you want to do?\n",
        "A: I want to get a puppy for my son.\n",
        "B: That will make him so happy.\n",
        "A: Yeah, we’ve discussed it many times. I think he’s ready now.\n",
        "B: That’s good. Raising a dog is a tough issue. Like having a baby ;-)\n",
        "A: I'll get him one of those little dogs.\n",
        "B: One that won't grow up too big;-)\n",
        "A: And eat too much;-))\n",
        "B: Do you know which one he would like?\n",
        "A: Oh, yes, I took him there last Monday. He showed me one that he really liked.\n",
        "B: I bet you had to drag him away.\n",
        "A: He wanted to take it home right away ;-).\n",
        "B: I wonder what he'll name it.\n",
        "A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\n",
        "---\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_NR3RLaW-2u",
        "outputId": "750dfa6f-5d32-48bc-813d-1cb84a5be6b1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summarize this dialog:\n",
            "A: Hi Tom, are you busy tomorrow’s afternoon?\n",
            "B: I’m pretty sure I am. What’s up?\n",
            "A: Can you go with me to the animal shelter?.\n",
            "B: What do you want to do?\n",
            "A: I want to get a puppy for my son.\n",
            "B: That will make him so happy.\n",
            "A: Yeah, we’ve discussed it many times. I think he’s ready now.\n",
            "B: That’s good. Raising a dog is a tough issue. Like having a baby ;-)\n",
            "A: I'll get him one of those little dogs.\n",
            "B: One that won't grow up too big;-)\n",
            "A: And eat too much;-))\n",
            "B: Do you know which one he would like?\n",
            "A: Oh, yes, I took him there last Monday. He showed me one that he really liked.\n",
            "B: I bet you had to drag him away.\n",
            "A: He wanted to take it home right away ;-).\n",
            "B: I wonder what he'll name it.\n",
            "A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\n",
            "---\n",
            "Summary:\n",
            "A: Hi Tom, are you busy tomorrow’s afternoon?\n",
            "B: I’m pretty sure I am. What’s up?\n",
            "A: Can you go with me to the animal shelter?.\n",
            "B: What do you want to do?\n",
            "A: I want to get a puppy for my son.\n",
            "B: That will make him so happy.\n",
            "A: Yeah, we’ve discussed it many times. I think he’s ready now.\n",
            "B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 微调\n",
        "\n",
        "\n",
        "在自然语言处理 (NLP) 中，PEFT（parameter-efficient fine-tuning） 是一种用于改进语言模型性能的技术。它通过在注意力层中添加一个矩阵来实现。该矩阵用于调整注意力权重，以便模型能够更好地理解句子中的关系。\n",
        "\n",
        "PEFT 分成3种方法\n",
        "* Prefix/Prompt-Tuning\n",
        "* Adapter-Tuning\n",
        "* LoRA\n",
        "\n",
        "下面的例子是采用LoRA的方式。\n",
        "* 需要用到A100，40g，才能完成这个微调。*"
      ],
      "metadata": {
        "id": "e1wkiQIdXRr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip 直接安装\n",
        "!pip install --extra-index-url https://download.pytorch.org/whl/test/cu118 llama-recipes\n"
      ],
      "metadata": {
        "id": "cjzXZHyadO6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!export CUDA_VISIBLE_DEVICES=0\n",
        "!python -m llama_recipes.finetuning  --use_peft --peft_method lora --quantization  \\\n",
        "--model_name {model_id} \\\n",
        "--output_dir {model_id}-peft"
      ],
      "metadata": {
        "id": "vlLP51LjZa6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#对微调后的模型进行推理测试\n",
        "import torch\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
        "from peft import PeftModel, PeftConfig"
      ],
      "metadata": {
        "id": "Xcj3l5FtfnAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id=\"/content/Llama-2-7b-hf-peft\"\n",
        "tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
        "model =LlamaForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16)"
      ],
      "metadata": {
        "id": "eNk6duQ2f-J5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 还没理解为啥需要再次加载\n",
        "model = PeftModel.from_pretrained(model, \"/content/Llama-2-7b-hf-peft\")"
      ],
      "metadata": {
        "id": "GJ7wmMy_iN9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 测试微调后的效果\n",
        "eval_prompt = \"\"\"\n",
        "Summarize this dialog:\n",
        "A: Hi Tom, are you busy tomorrow’s afternoon?\n",
        "B: I’m pretty sure I am. What’s up?\n",
        "A: Can you go with me to the animal shelter?.\n",
        "B: What do you want to do?\n",
        "A: I want to get a puppy for my son.\n",
        "B: That will make him so happy.\n",
        "A: Yeah, we’ve discussed it many times. I think he’s ready now.\n",
        "B: That’s good. Raising a dog is a tough issue. Like having a baby ;-)\n",
        "A: I'll get him one of those little dogs.\n",
        "B: One that won't grow up too big;-)\n",
        "A: And eat too much;-))\n",
        "B: Do you know which one he would like?\n",
        "A: Oh, yes, I took him there last Monday. He showed me one that he really liked.\n",
        "B: I bet you had to drag him away.\n",
        "A: He wanted to take it home right away ;-).\n",
        "B: I wonder what he'll name it.\n",
        "A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\n",
        "---\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "CLWKKbXygMSY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}