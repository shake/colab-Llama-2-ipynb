{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPXchrNqGYoDoV1rcPTfYCE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shake/colab-Llama-2-ipynb/blob/main/Llama_2_GPTQ%E9%87%8F%E5%8C%96.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[参考文章：大卫](https://mp.weixin.qq.com/s/BYhJcmrE9rxyHvxS6-Ph)"
      ],
      "metadata": {
        "id": "UXLtFrZqJe2s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElNgnggI7cZy"
      },
      "outputs": [],
      "source": [
        "# 安装需要依赖\n",
        "!pip install -q git+https://github.com/huggingface/transformers\n",
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2  trl==0.4.7\n",
        "!pip install -q huggingface_hub sentencepiece einops\n",
        "!pip install -q optimum==1.12.0 auto-gptq==0.4.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import 密钥，token\n",
        "from google.colab import userdata\n",
        "hf_token = userdata.get('huggingface')\n",
        "!git config --global credential.helper store\n",
        "!huggingface-cli login --token $hf_token --add-to-git-credential"
      ],
      "metadata": {
        "id": "7kAkvRaA8o2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset id from Hugging Face\n",
        "dataset_id = \"wikitext2\""
      ],
      "metadata": {
        "id": "VvxnHqCO98ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from optimum.gptq import GPTQQuantizer\n",
        "\n",
        "# GPTQ quantizer\n",
        "quantizer = GPTQQuantizer(bits=4, dataset=dataset_id, model_seqlen=4096)\n",
        "quantizer.quant_method = \"gptq\""
      ],
      "metadata": {
        "id": "NK5MQIEA91gT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Hugging Face model id\n",
        "model_id = \"chenshake/Llama-2-7b-hf\"\n",
        "\n",
        " # bug with fast tokenizer we load the model in fp16 on purpose\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, low_cpu_mem_usage=True, torch_dtype=torch.float16)"
      ],
      "metadata": {
        "id": "7ZHTgKC5-Ib1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "\n",
        "# quantize the model\n",
        "quantized_model = quantizer.quantize_model(model, tokenizer)\n",
        "\n",
        "\n",
        "# save the quantize model to disk\n",
        "save_folder = \"quantized_llama\"\n",
        "quantized_model.save_pretrained(save_folder, safe_serialization=True)\n",
        "\n",
        "\n",
        "# load fresh, fast tokenizer and save it to disk\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id).save_pretrained(save_folder)\n",
        "\n",
        "\n",
        "# save quantize_config.json for TGI\n",
        "with open(os.path.join(save_folder, \"quantize_config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "  quantizer.disable_exllama = False\n",
        "  json.dump(quantizer.to_dict(), f, indent=2)"
      ],
      "metadata": {
        "id": "WG6XCHgaA_uK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 我们希望利用 exllama 内核，因此需要更改 `config.json。\n",
        "with open(os.path.join(save_folder, \"config.json\"), \"r\", encoding=\"utf-8\") as f:\n",
        "  config = json.load(f)\n",
        "  config[\"quantization_config\"][\"disable_exllama\"] = False\n",
        "with open(os.path.join(save_folder, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(config, f, indent=2)\n"
      ],
      "metadata": {
        "id": "NoNyMTLWC1YF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fc24RCYCDmpn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}