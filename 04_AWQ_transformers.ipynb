{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shake/colab-Llama-2-ipynb/blob/main/04_AWQ_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AWQ quantization (AutoAWQ ) for lighter and faster quantized inference of LLMs"
      ],
      "metadata": {
        "id": "H_D9kG_efts3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In June 2023, the [AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration](https://arxiv.org/pdf/2306.00978.pdf) has been published by Ji Lin et al. The paper details an algorithm to compress any transformer-based language model in few bits with a tiny performance degradation. To learn more about this quantization method, Professor Song Han gives a excellent [talk](https://hanlab.mit.edu/projects/awq).\n",
        "\n",
        "We new support loading models that are quantized with GPTQ algorithm in ðŸ¤— transformers from two different libraries: [LLM-AWQ](https://github.com/mit-han-lab/llm-awq) and [AutoAWQ](https://github.com/casper-hansen/AutoAWQ).\n",
        "\n",
        "Let's check in this notebook the different options (quantize a model, push a quantized model on the ðŸ¤— Hub, load an already quantized model from the Hub, etc.) that are offered in this integration!"
      ],
      "metadata": {
        "id": "Wwsg6nCwoThm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## check colab\n",
        "\n"
      ],
      "metadata": {
        "id": "rzdrF6Xp8-Bw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AutoAWQ maybe need 3.6 transformers, colab is 3.5ï¼Œneed updateï¼Œand restart runtimeã€‚\n",
        "\n"
      ],
      "metadata": {
        "id": "eAgft4pHZgiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check default cuda version\n",
        "!nvcc --version 4\n"
      ],
      "metadata": {
        "id": "COhAMmQemLmE",
        "outputId": "ec467134-bbff-4b31-d166-e57e1f7a5d76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check default torch version\n",
        "import torch\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "id": "UBTEIwGMkN_K",
        "outputId": "57c7cb05-28e4-4064-f546-cb6ed201eff1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check default transformers version\n",
        "import transformers\n",
        "print (transformers.__version__)"
      ],
      "metadata": {
        "id": "G6YIUbk8kPqI",
        "outputId": "61d94e6e-1304-49ac-a9b4-092bcfdeba05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.35.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U git+https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "id": "0fj59QpVnWKY",
        "outputId": "57e9115e-afd0-4bf0-d2f1-d2cdcc976b7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remenber runtime--restart session\n",
        "import transformers\n",
        "print (transformers.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghTY3oSr3zSF",
        "outputId": "3082a156-5b24-4b8f-82ed-75cf091b2bd7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.37.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AWQ"
      ],
      "metadata": {
        "id": "L2ZTO3OSoL5T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cY3RD1iKfKPD"
      },
      "outputs": [],
      "source": [
        "!pip install -q  autoawq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make a soft link for  /root/.cache\n",
        "!mkdir -p /root/.cache/huggingface\n",
        "!ln -s /root/.cache/huggingface /content"
      ],
      "metadata": {
        "id": "mDnUWCMsznZU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "need to login hugging face, download meta Llama-2-7b and upload quantized models"
      ],
      "metadata": {
        "id": "VMRaN_u2TcCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# login huggingface\n",
        "! git config --global credential.helper store\n",
        "!huggingface-cli login --token hf_ziljuWSLzXXrlDvr --add-to-git-credential"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JRigPxq4xhJ",
        "outputId": "2a488279-fe51-465b-d558-64046dd93050"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token is valid (permission: read).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**quantized model**"
      ],
      "metadata": {
        "id": "GuN7S8NC487f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from awq import AutoAWQForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_path = 'meta-llama/Llama-2-7b-hf'\n",
        "quant_path = 'Llama-2-7b-awq'\n",
        "quant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }\n",
        "\n",
        "# Load model\n",
        "model = AutoAWQForCausalLM.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "\n",
        "# Quantize\n",
        "model.quantize(tokenizer, quant_config=quant_config)\n",
        "\n",
        "# Save quantized model\n",
        "model.save_quantized(quant_path)\n",
        "tokenizer.save_pretrained(quant_path)"
      ],
      "metadata": {
        "id": "Qn_P_E5p7gAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quant_config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVzKDBlP_6MV",
        "outputId": "b61b9cf6-4da4-4c6a-a6b9-b6dd1a711307"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'zero_point': True, 'q_group_size': 128, 'w_bit': 4, 'version': 'GEMM'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## compatible with transformers\n",
        "\n",
        " In order to make it compatible with transformers, we need to modify the config file."
      ],
      "metadata": {
        "id": "PuPLq9sa8EaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AwqConfig, AutoConfig\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "# modify the config file so that it is compatible with transformers integration\n",
        "quantization_config = AwqConfig(\n",
        "    bits=quant_config[\"w_bit\"],\n",
        "    group_size=quant_config[\"q_group_size\"],\n",
        "    zero_point=quant_config[\"zero_point\"],\n",
        "    version=quant_config[\"version\"].lower(),\n",
        ").to_dict()\n",
        "\n",
        "# the pretrained transformers model is stored in the model attribute + we need to pass a dict\n",
        "model.model.config.quantization_config = quantization_config\n",
        "# a second solution would be to use Autoconfig and push to hub (what we do at llm-awq)\n",
        "\n",
        "\n",
        "# save model weights\n",
        "model.save_quantized(quant_path)\n",
        "tokenizer.save_pretrained(quant_path)"
      ],
      "metadata": {
        "id": "KE8xjwlL8DnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## upload models to Huggingface"
      ],
      "metadata": {
        "id": "VD9z5t7XRd5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api = HfApi()\n",
        "api.upload_folder(\n",
        "    folder_path=\"Llama-2-7b-awq\",\n",
        "    repo_id=\"chenshake/Llama-2-7b-awq\",\n",
        "    repo_type=\"model\",\n",
        ")"
      ],
      "metadata": {
        "id": "RWIh6iUR_i5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test\n",
        "\n",
        "Now we can use our model with transformers library to run inference !"
      ],
      "metadata": {
        "id": "sK6McF3_CL1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"chenshake/Llama-2-7b-awq\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"chenshake/Llama-2-7b-awq\").to(0)\n",
        "\n",
        "text = \"Hello my name is\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
        "\n",
        "out = model.generate(**inputs, max_new_tokens=5)\n",
        "print(tokenizer.decode(out[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "Z0hAXYanCDW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading  model from huggingface on Google colab"
      ],
      "metadata": {
        "id": "P15qU3aMq_op"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"chenshake/Llama-2-7b-awq\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\")"
      ],
      "metadata": {
        "id": "DvZj4YgVCRg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"User:\\nHello can you provide me with top-3 cool places to visit in Paris?\\n\\nAssistant:\\n\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
        "\n",
        "out = model.generate(**inputs, max_new_tokens=300)\n",
        "print(tokenizer.decode(out[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "aF24pWzltQna"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}