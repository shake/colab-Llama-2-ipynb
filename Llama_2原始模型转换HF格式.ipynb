{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPBxfqfFLUGDk6j+FiK4GBP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shake/colab-Llama-2-ipynb/blob/main/Llama_2%E5%8E%9F%E5%A7%8B%E6%A8%A1%E5%9E%8B%E8%BD%AC%E6%8D%A2HF%E6%A0%BC%E5%BC%8F.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "原始模型的转换，这个我对着文档操作过无数次，已经很熟练，这次计划把格式转换的模型，上传到hugingface上，日后对模型是微调，就从自己转换的HF格式，开始进行微调。\n",
        "\n",
        "另外一个选择，就是直接使用Facebook提供的HF格式。\n",
        "\n",
        "**重点**\n",
        "\n",
        "* colab，你需要High-RAM才能完成转换。\n",
        "* 只需要修改**MODEL_ID** 就可以，支持Llama-2 所有版本。\n",
        "* colab保存huggingface的token，如果需要上传，需要写入权限的token\n",
        "* 上传转换的模型到hugingface\n",
        "* 7B是13G，13B是26G，70B是140G。进行转换，需要磁盘至少有3倍的剩余空间。"
      ],
      "metadata": {
        "id": "QGRA-l595Q8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 输入你需要转换的Llama版本，username 是你登录hugingface的用户名\n",
        "MODEL_ID = \"meta-llama/Llama-2-7b\"\n",
        "MODEL_NAME = MODEL_ID.split('/')[-1]\n",
        "NEW_MODEL = MODEL_NAME+'-hf'\n",
        "Parameter = str.upper(MODEL_NAME.split('-')[2])\n"
      ],
      "metadata": {
        "id": "LoAFMrIa7Ctl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Parameter)"
      ],
      "metadata": {
        "id": "R3seKR1I-5Yd",
        "outputId": "71814de4-1a3e-47b7-f83a-697e7bbfc759",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 安装需要依赖\n",
        "%%capture\n",
        "!pip install -q git+https://github.com/huggingface/transformers\n",
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2  trl==0.4.7\n",
        "!pip install -q huggingface_hub sentencepiece"
      ],
      "metadata": {
        "id": "LCrzToQi5Qhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgrDL-3H48wC"
      },
      "outputs": [],
      "source": [
        "# import 密钥，token\n",
        "%%capture\n",
        "from google.colab import userdata\n",
        "hf_token = userdata.get('huggingface')\n",
        "!git config --global credential.helper store\n",
        "!huggingface-cli login --token $hf_token --add-to-git-credential"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import huggingface_hub\n",
        "username = f'{huggingface_hub.whoami()[\"name\"]}'\n",
        "print(username)"
      ],
      "metadata": {
        "id": "vIrE4_AWufyW",
        "outputId": "0fc92ba1-2d26-44a3-c27b-65d03b3f6a1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chenshake\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 可以整个目录进行下载，这里专门单独选择文件来下载 ,去掉--quiet参数，可以看到详细的下载过程。\n",
        "!huggingface-cli download --quiet\\\n",
        "\t--local-dir=./$MODEL_NAME \\\n",
        "\t$MODEL_ID \\\n",
        "  --include \"*.json\"   \"*.model\" \"*.chk\" \"*.pth\""
      ],
      "metadata": {
        "id": "mQqlEWQsyIKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 查看下载结果,文件是下载到cache，做了一个链接到目录下，所以无法直接查看文件大小。\n",
        "import os.path\n",
        "from pathlib import Path\n",
        "path=os.path.join(MODEL_NAME)\n",
        "print(path)\n",
        "!ls -lsh ./$MODEL_NAME"
      ],
      "metadata": {
        "id": "7KQQheK8XcN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 格式转换需要提前创建好目录\n",
        "!mkdir ./$MODEL_NAME/$Parameter\n",
        "!cp ./$MODEL_NAME/params.json ./$MODEL_NAME/$Parameter/params.json"
      ],
      "metadata": {
        "id": "PHwwMb0E797E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ./$MODEL_NAME/$Parameter"
      ],
      "metadata": {
        "id": "C0vNMS-Yun7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# update cache，有时候会出现一个transformers的 cache 的错误提示，提前运行，解决这个问题。\n",
        "from transformers.utils.hub import move_cache"
      ],
      "metadata": {
        "id": "XErsFdNc8J40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#解决colab字符集错误，遇到错误再运行也是可以。\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "8QsFGUCP8eux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 下载HF格式转换工具\n",
        "!wget -q https://raw.githubusercontent.com/huggingface/transformers/main/src/transformers/models/llama/convert_llama_weights_to_hf.py"
      ],
      "metadata": {
        "id": "tvM4XT6m73i_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# 开始转换, 消耗内存超过16G，默认内存，会导致中断。\n",
        "!python convert_llama_weights_to_hf.py --model_size $Parameter \\\n",
        "    --input_dir ./$MODEL_NAME \\\n",
        "    --output_dir ./$NEW_MODEL"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "DoOKOITYWF-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 调用api，在你的hugingface账号下，创建repo，存放模型，记得修改上面的用户名的变量。\n",
        "from huggingface_hub import create_repo, HfApi\n",
        "\n",
        "# Create empty repo\n",
        "create_repo(\n",
        "    repo_id=f\"{username}/{NEW_MODEL}\",\n",
        "    repo_type=\"model\",\n",
        "    exist_ok=True,\n",
        ")"
      ],
      "metadata": {
        "id": "kH8-tPHx8fll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 把转换HF格式，上传到huggingface,你账号的model里。\n",
        "api = HfApi()\n",
        "api.upload_folder(\n",
        "\tfolder_path=f\"{NEW_MODEL}\",\n",
        "\trepo_id=f\"{username}/{NEW_MODEL}\",\n",
        "\trepo_type=\"model\",\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "2nxiV7xygqGc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}