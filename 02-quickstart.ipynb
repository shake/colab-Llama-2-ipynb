{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shake/colab-Llama-2-ipynb/blob/main/02-quickstart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGsrQsJYDhiU"
      },
      "source": [
        "## Quick Start Notebook\n",
        "\n",
        "This notebook shows how to train a Llama 2 model on a single GPU (e.g. A10 with 24GB) using int8 quantization and LoRA.\n",
        "\n",
        "### Step 0: Install pre-requirements and convert checkpoint\n",
        "\n",
        "The example uses the Hugging Face trainer and model which means that the checkpoint has to be converted from its original format into the dedicated Hugging Face format.\n",
        "The conversion can be achieved by running the `convert_llama_weights_to_hf.py` script provided with the transformer package.\n",
        "Given that the original checkpoint resides under `models/7B` we can install all requirements and convert the checkpoint with:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "此笔记本展示了如何使用 int8 量化和 LoRA 在单个 GPU（例如 24GB 的 A10）上训练 Llama 2 模型。\n",
        "\n",
        "该示例使用 Hugging Face 训练器和模型，这意味着检查点必须从其原始格式转换为专用的 Hugging Face 格式。可以通过运行转换器包提供的 convert_llama_weights_to_hf.py 脚本来实现转换。鉴于原始检查点位于以下位置，我们可以安装所有要求并使用以下 models/7B 命令转换检查点："
      ],
      "metadata": {
        "id": "E7TIqsKiGFFe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjH_JhuLDhiV"
      },
      "outputs": [],
      "source": [
        "# %%bash\n",
        "# pip install llama-recipes transformers datasets accelerate sentencepiece protobuf==3.20 py7zr scipy peft bitsandbytes fire torch_tb_profiler ipywidgets\n",
        "# TRANSFORM=`python -c \"import transformers;print('/'.join(transformers.__file__.split('/')[:-1])+'/models/llama/convert_llama_weights_to_hf.py')\"`\n",
        "# python ${TRANSFORM} --input_dir models --model_size 7B --output_dir models_hf/7B"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 安装需要的包\n",
        "!pip -q install gradio huggingface_hub\n",
        "\n",
        "# import\n",
        "import os\n",
        "import shutil\n",
        "import huggingface_hub as hh\n",
        "import pandas as pd\n",
        "\n",
        "# 下载llama 2，需要使用HuggingFace的token通过验证才能下载，其他模型，这一步可以省掉。\n",
        "# 配置git存储密钥\n",
        "! git config --global credential.helper store\n",
        "!huggingface-cli login --token hf_FqOyPDAURgkbG --add-to-git-credential\n",
        "\n",
        "# 下载模型，设置huggingface的repo_id，更换不同的模型，\n",
        "# 只需要在repo_id设置就可以。其他地方无需调整。\n",
        "repo_id = \"meta-llama/Llama-2-7b\"\n",
        "repo_name = repo_id.replace(\"/\",\"---\")\n",
        "\n",
        "# 定义容量显示和下载路径\n",
        "\n",
        "def format_size(bytes, precision=2):\n",
        "\t\"\"\"\n",
        "\tConvert a file size in bytes to a human-readable format like KB, MB, GB, etc.\n",
        "\tHuggingface use 1000 not 1024\n",
        "\t\"\"\"\n",
        "\tunits = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n",
        "\tsize = float(bytes)\n",
        "\tindex = 0\n",
        "\n",
        "\twhile size >= 1000 and index < len(units) - 1:\n",
        "\t\tindex += 1\n",
        "\t\tsize /= 1000\n",
        "\n",
        "\treturn f\"{size:.{precision}f} {units[index]}\"\n",
        "\n",
        "\n",
        "def list_repo_files_info(repo_id,token=None):\n",
        "\tdata_ls = []\n",
        "\tfor file in list(hh.list_files_info(repo_id)):\n",
        "\t\tdata_ls.append([file.path,format_size(file.size)])\n",
        "\tfiles = [file[0] for file in data_ls]\n",
        "\tdata = pd.DataFrame(data_ls,columns = ['文件名','大小'])\n",
        "\treturn data, files\n",
        "\n",
        "# 模型下载到当前目录下的\"./download\"目录\n",
        "def download_file(repo_id,filenames):\n",
        "\tprint(filenames)\n",
        "\trepo_name = repo_id.replace(\"/\",\"---\")\n",
        "\n",
        "\tfor filename in filenames:\n",
        "\t\tprint(filename)\n",
        "\t\tout = hh.hf_hub_download(repo_id=repo_id,filename=filename,local_dir=f\"./download/{repo_name}\",local_dir_use_symlinks=False,force_download =True)\n",
        "\tout_path = f\"./download/{repo_name}\"\n",
        "\treturn out_path\n",
        "\n",
        "# 查看模型的文件\n",
        "data, filenames = list_repo_files_info(repo_id)\n",
        "filenames\n",
        "\n",
        "# 开始下载模型\n",
        "out_path = download_file(repo_id,filenames)"
      ],
      "metadata": {
        "id": "7Yw4xnHnACVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/huggingface/transformers/main/src/transformers/models/llama/convert_llama_weights_to_hf.py\n",
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7 huggingface_hub sentencepiece"
      ],
      "metadata": {
        "id": "4O2J0i8IAUEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/download/meta-llama---Llama-2-7b /content/Llama-2-7b\n",
        "!mkdir /content/Llama-2-7b/7B\n",
        "!cp /content/Llama-2-7b/params.json /content/Llama-2-7b/7B/params.json"
      ],
      "metadata": {
        "id": "_-YxCFlyAVQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls /content/Llama-2-7b/7B/"
      ],
      "metadata": {
        "id": "NWTFwq5UAfrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.utils.hub import move_cache"
      ],
      "metadata": {
        "id": "Jio6xxPCAh85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 开始从llama 原始格式转换成hf格式，\n",
        "!python3 convert_llama_weights_to_hf.py \\\n",
        "    --input_dir ./Llama-2-7b  --model_size 7B --output_dir ./Llama-2-7b-hf"
      ],
      "metadata": {
        "id": "v8bIFyICAlB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#解决colab字符集错误\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "hSHGKLlLBtH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -q gekko pandas\n",
        "!git clone https://github.com/PanQiWei/AutoGPTQ"
      ],
      "metadata": {
        "id": "dVww5o5GCL1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd AutoGPTQ\n",
        "!pwd\n"
      ],
      "metadata": {
        "id": "MQFAJKdECdbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install ."
      ],
      "metadata": {
        "id": "JHKkXa-zCxwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtqS2AljDoOu",
        "outputId": "22ff8d67-3d18-440c-e9da-bbb5735dcee8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://gist.githubusercontent.com/TheBloke/b47c50a70dd4fe653f64a12928286682/raw/ebcee019d90a178ee2e6a8107fdd7602c8f1192a/quant_autogptq.py"
      ],
      "metadata": {
        "id": "My0w0Sy4GJkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9NyKF1JFjQH",
        "outputId": "21005d35-151d-406d-a78a-bf0c1abbde34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AutoGPTQ\t\t\tdownload    Llama-2-7b-hf      sample_data\n",
            "convert_llama_weights_to_hf.py\tLlama-2-7b  quant_autogptq.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 quant_autogptq.py ./Llama-2-7b-hf ./llama-2-7b-hf-gptq \\\n",
        "wikitext --bits 4 --group_size 128 --desc_act 0 --damp 0.1 \\\n",
        "--dtype float16 --seqlen 4096 --num_samples 128 --use_fast"
      ],
      "metadata": {
        "id": "FFA09gb7D0Px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axhsX5mZJUQg",
        "outputId": "a45547a5-1dd0-4218-f563-636945759ce2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AutoGPTQ\t\t\tdownload    Llama-2-7b-hf\tquant_autogptq.py\n",
            "convert_llama_weights_to_hf.py\tLlama-2-7b  llama-2-7b-hf-gptq\tsample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!du -sh ./llama-2-7b-hf-gptq"
      ],
      "metadata": {
        "id": "FVedYdzmHwvP",
        "outputId": "798869bd-8553-4e35-e5c0-9dd5f3849479",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.7G\t./llama-2-7b-hf-gptq\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tuZtFu-DhiW"
      },
      "source": [
        "### Step 1: Load the model\n",
        "\n",
        "Point model_id to model weight folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVmP1yUwDhiX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
        "\n",
        "model_id=\"./Llama-2-7b-hf\"\n",
        "\n",
        "tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
        "\n",
        "model =LlamaForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYrvUKzYDhiX"
      },
      "source": [
        "### Step 2: Load the preprocessed dataset\n",
        "\n",
        "We load and preprocess the samsum dataset which consists of curated pairs of dialogs and their summarization:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --extra-index-url https://download.pytorch.org/whl/test/cu118 llama-recipes\n"
      ],
      "metadata": {
        "id": "sTAHluNiIRr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5K_nxVKDhiY"
      },
      "outputs": [],
      "source": [
        "from llama_recipes.utils.dataset_utils import get_preprocessed_dataset\n",
        "from llama_recipes.configs.datasets import samsum_dataset\n",
        "\n",
        "train_dataset = get_preprocessed_dataset(tokenizer, samsum_dataset, 'train')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1w2FIJYXDhiY"
      },
      "source": [
        "### Step 3: Check base model\n",
        "\n",
        "Run the base model on an example input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rz-aJwIYDhiZ",
        "outputId": "cc69f085-126f-476c-dc17-14bf8519b6e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summarize this dialog:\n",
            "A: Hi Tom, are you busy tomorrow’s afternoon?\n",
            "B: I’m pretty sure I am. What’s up?\n",
            "A: Can you go with me to the animal shelter?.\n",
            "B: What do you want to do?\n",
            "A: I want to get a puppy for my son.\n",
            "B: That will make him so happy.\n",
            "A: Yeah, we’ve discussed it many times. I think he’s ready now.\n",
            "B: That’s good. Raising a dog is a tough issue. Like having a baby ;-)\n",
            "A: I'll get him one of those little dogs.\n",
            "B: One that won't grow up too big;-)\n",
            "A: And eat too much;-))\n",
            "B: Do you know which one he would like?\n",
            "A: Oh, yes, I took him there last Monday. He showed me one that he really liked.\n",
            "B: I bet you had to drag him away.\n",
            "A: He wanted to take it home right away ;-).\n",
            "B: I wonder what he'll name it.\n",
            "A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\n",
            "---\n",
            "Summary:\n",
            "A: Hi Tom, are you busy tomorrow’s afternoon?\n",
            "B: I’m pretty sure I am. What’s up?\n",
            "A: Can you go with me to the animal shelter?.\n",
            "B: What do you want to do?\n",
            "A: I want to get a puppy for my son.\n",
            "B: That will make him so happy.\n",
            "A: Yeah, we’ve discussed it many times. I think he’s ready now.\n",
            "B\n"
          ]
        }
      ],
      "source": [
        "eval_prompt = \"\"\"\n",
        "Summarize this dialog:\n",
        "A: Hi Tom, are you busy tomorrow’s afternoon?\n",
        "B: I’m pretty sure I am. What’s up?\n",
        "A: Can you go with me to the animal shelter?.\n",
        "B: What do you want to do?\n",
        "A: I want to get a puppy for my son.\n",
        "B: That will make him so happy.\n",
        "A: Yeah, we’ve discussed it many times. I think he’s ready now.\n",
        "B: That’s good. Raising a dog is a tough issue. Like having a baby ;-)\n",
        "A: I'll get him one of those little dogs.\n",
        "B: One that won't grow up too big;-)\n",
        "A: And eat too much;-))\n",
        "B: Do you know which one he would like?\n",
        "A: Oh, yes, I took him there last Monday. He showed me one that he really liked.\n",
        "B: I bet you had to drag him away.\n",
        "A: He wanted to take it home right away ;-).\n",
        "B: I wonder what he'll name it.\n",
        "A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\n",
        "---\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNViKyWmDhia"
      },
      "source": [
        "We can see that the base model only repeats the conversation.\n",
        "\n",
        "### Step 4: Prepare model for PEFT\n",
        "\n",
        "Let's prepare the model for Parameter Efficient Fine Tuning (PEFT):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKxA94lbDhia",
        "outputId": "25da0592-4806-4762-e792-0c7b8dd1f1fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model.train()\n",
        "\n",
        "def create_peft_config(model):\n",
        "    from peft import (\n",
        "        get_peft_model,\n",
        "        LoraConfig,\n",
        "        TaskType,\n",
        "        prepare_model_for_int8_training,\n",
        "    )\n",
        "\n",
        "    peft_config = LoraConfig(\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "        inference_mode=False,\n",
        "        r=8,\n",
        "        lora_alpha=32,\n",
        "        lora_dropout=0.05,\n",
        "        target_modules = [\"q_proj\", \"v_proj\"]\n",
        "    )\n",
        "\n",
        "    # prepare int-8 model for training\n",
        "    model = prepare_model_for_int8_training(model)\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    model.print_trainable_parameters()\n",
        "    return model, peft_config\n",
        "\n",
        "# create peft config\n",
        "model, lora_config = create_peft_config(model)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "myC2G4CJDhib"
      },
      "source": [
        "### Step 5: Define an optional profiler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Kyc3ZOiDhib"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainerCallback\n",
        "from contextlib import nullcontext\n",
        "enable_profiler = False\n",
        "output_dir = \"tmp/llama-output\"\n",
        "\n",
        "config = {\n",
        "    'lora_config': lora_config,\n",
        "    'learning_rate': 1e-4,\n",
        "    'num_train_epochs': 1,\n",
        "    'gradient_accumulation_steps': 2,\n",
        "    'per_device_train_batch_size': 2,\n",
        "    'gradient_checkpointing': False,\n",
        "}\n",
        "\n",
        "# Set up profiler\n",
        "if enable_profiler:\n",
        "    wait, warmup, active, repeat = 1, 1, 2, 1\n",
        "    total_steps = (wait + warmup + active) * (1 + repeat)\n",
        "    schedule =  torch.profiler.schedule(wait=wait, warmup=warmup, active=active, repeat=repeat)\n",
        "    profiler = torch.profiler.profile(\n",
        "        schedule=schedule,\n",
        "        on_trace_ready=torch.profiler.tensorboard_trace_handler(f\"{output_dir}/logs/tensorboard\"),\n",
        "        record_shapes=True,\n",
        "        profile_memory=True,\n",
        "        with_stack=True)\n",
        "\n",
        "    class ProfilerCallback(TrainerCallback):\n",
        "        def __init__(self, profiler):\n",
        "            self.profiler = profiler\n",
        "\n",
        "        def on_step_end(self, *args, **kwargs):\n",
        "            self.profiler.step()\n",
        "\n",
        "    profiler_callback = ProfilerCallback(profiler)\n",
        "else:\n",
        "    profiler = nullcontext()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Exs8QDB-Dhib"
      },
      "source": [
        "### Step 6: Fine tune the model\n",
        "\n",
        "Here, we fine tune the model for a single epoch which takes a bit more than an hour on a A100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waDBu1HSDhib",
        "outputId": "b9fad7e8-1bf6-4e42-ccbf-2cd0a1bdd32a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='389' max='389' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [389/389 1:08:12, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.942400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.825100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.782300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.750400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.721500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.691900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.696800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.701100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.679200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.689800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.692000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.667800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.657600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.692300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.685400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.687100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>1.655800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>1.673500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>1.686500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.684200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>1.656600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>1.637600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>1.658900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>1.660200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.678200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>1.673000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>1.642700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>1.684200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>1.658800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.658800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>1.651800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>1.638800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>1.678800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>1.671900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.702600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>1.637100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>1.672600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>1.654300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import default_data_collator, Trainer, TrainingArguments\n",
        "\n",
        "\n",
        "\n",
        "# Define training args\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=True,\n",
        "    bf16=True,  # Use BF16 if available\n",
        "    # logging strategies\n",
        "    logging_dir=f\"{output_dir}/logs\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"no\",\n",
        "    optim=\"adamw_torch_fused\",\n",
        "    max_steps=total_steps if enable_profiler else -1,\n",
        "    **{k:v for k,v in config.items() if k != 'lora_config'}\n",
        ")\n",
        "\n",
        "with profiler:\n",
        "    # Create Trainer instance\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        data_collator=default_data_collator,\n",
        "        callbacks=[profiler_callback] if enable_profiler else [],\n",
        "    )\n",
        "\n",
        "    # Start training\n",
        "    trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "329cqhIYDhib"
      },
      "source": [
        "### Step 7:\n",
        "Save model checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYKS3W1eDhib"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYVyCrNKDhic"
      },
      "source": [
        "### Step 8:\n",
        "Try the fine tuned model on the same example again to see the learning progress:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Kt1eYi-Dhic",
        "outputId": "f54b9a29-31c8-4031-9fe5-13c175f4e3c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summarize this dialog:\n",
            "A: Hi Tom, are you busy tomorrow’s afternoon?\n",
            "B: I’m pretty sure I am. What’s up?\n",
            "A: Can you go with me to the animal shelter?.\n",
            "B: What do you want to do?\n",
            "A: I want to get a puppy for my son.\n",
            "B: That will make him so happy.\n",
            "A: Yeah, we’ve discussed it many times. I think he’s ready now.\n",
            "B: That’s good. Raising a dog is a tough issue. Like having a baby ;-)\n",
            "A: I'll get him one of those little dogs.\n",
            "B: One that won't grow up too big;-)\n",
            "A: And eat too much;-))\n",
            "B: Do you know which one he would like?\n",
            "A: Oh, yes, I took him there last Monday. He showed me one that he really liked.\n",
            "B: I bet you had to drag him away.\n",
            "A: He wanted to take it home right away ;-).\n",
            "B: I wonder what he'll name it.\n",
            "A: He said he’d name it after his dead hamster – Lemmy  - he's  a great Motorhead fan :-)))\n",
            "---\n",
            "Summary:\n",
            "A wants to get a puppy for his son. He took him to the animal shelter last Monday. He showed him one that he really liked. He wants to name it after his dead hamster - Lemmy.\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "vscode": {
      "interpreter": {
        "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
      }
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}