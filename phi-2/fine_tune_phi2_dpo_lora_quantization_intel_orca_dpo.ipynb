{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shake/colab-Llama-2-ipynb/blob/main/phi-2/fine_tune_phi2_dpo_lora_quantization_intel_orca_dpo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "busfcvZ4Uu-F"
      },
      "source": [
        "# Fine-tune Phi2 using DPO LoRA and quantization\n",
        "\n",
        "How to get a DPO dataset? You can either [create synthethic data or review a existing dataset with distilabel](https://huggingface.co/argilla/distilabeled-Hermes-2.5-Mistral-7B) or use a completely raw approach and start [with some existing data collection as obtained from this Jupyter Notebook.](https://colab.research.google.com/drive/1p7d-iqtKlxojT3xetEL6PsJjdhZcm1xK?usp=sharing)\n",
        "\n",
        "After the annotators have submitted their feedback, we will use it to fine-tune [microsoft/phi-2](https://huggingface.co/microsoft/phi-2) for DPO. This model, known as Phi-2, is a scaled-down machine learning model with 2.7 billion parameters. Despite its smaller size, it excels in performance relative to larger models. Phi-2 has not been fine-tuned using DPO to align it with social reasoning.\n",
        "\n",
        "Install the Argilla client and the required third party libraries using pip:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJrcUCKs7X9S",
        "outputId": "1f324e79-c118-4219-c6e4-d64634b85bdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.9/150.9 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.7/79.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install bitsandbytes transformers peft accelerate datasets wandb trl -q -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK71X6lUWMEo"
      },
      "source": [
        "Let’s make the necessary imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "te74DElZ8r5t"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from typing import Dict, Any, Iterator, Tuple\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        ")\n",
        "import huggingface_hub\n",
        "import wandb\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "from trl import DPOTrainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wtz-pKwNWO6b"
      },
      "source": [
        "Let's login on Hugging Face to be able to upload our model after."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mogP0Ui7MVY"
      },
      "outputs": [],
      "source": [
        "huggingface_hub.login(token=userdata.get(\"HF_AUTH_TOKEN‡\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ex4kF6e4zHu",
        "outputId": "43e05259-5da0-4cd2-cf5e-2111fc342c6b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdavid_from_argilla\u001b[0m (\u001b[33margilla-io\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "wandb_token = userdata.get(\"WANDB_AUTH_TOKEN\")\n",
        "if wandb_token:\n",
        "    wandb.login(key=wandb_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F12EpUpbg6Fp"
      },
      "source": [
        "# Setup compute device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Px833HtTg5Rk",
        "outputId": "b8158b2c-a1b2-446d-d9b8-61b7d789d122"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using NVIDIA A100 80GB PCIe\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"Using {torch.cuda.get_device_name(0)}\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"No GPU available, using CPU instead.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDBxWthrI3n8"
      },
      "source": [
        "### Load the Intel Orca DPO dataset and prepare it\n",
        "\n",
        "We will load the [distilabeled Intel Orca DPO](https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs?row=0) from Argilla and prepare it for fine-tuning. In order to ensure data alignment with the pre-training, we will update the prompt template to match the original format of the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BE7djCZ2_Qdf",
        "outputId": "da000b9f-b79c-4051-e135-c9a5048ff73c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['system', 'input', 'chosen', 'rejected', 'generations', 'order', 'labelling_model', 'labelling_prompt', 'raw_labelling_response', 'rating', 'rationale', 'status', 'original_chosen', 'original_rejected', 'chosen_score', 'in_gsm8k_train'],\n",
              "    num_rows: 12859\n",
              "})"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "org_name = \"argilla\"\n",
        "dataset_name = \"distilabel-intel-orca-dpo-pairs\"\n",
        "dataset = load_dataset(f\"{org_name}/{dataset_name}\")\n",
        "dataset[\"train\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NoAFmSxiRiu"
      },
      "outputs": [],
      "source": [
        "# Indicate the template for the feedback task\n",
        "template = \"\"\"\\\n",
        "Instruct: {instruction}\\n\n",
        "Output: {response}\"\"\"\n",
        "\n",
        "def formatting_func(sample: Dict[str, Any]) -> Iterator[Tuple[str, str]]:\n",
        "    # Our annotators were asked to provide new responses, which we assume are better than the originals\n",
        "    sample[\"prompt\"] = template.format(instruction=sample[\"input\"], response=\"\")\n",
        "\n",
        "    return sample\n",
        "\n",
        "formatted_dataset = dataset.map(formatting_func).select_columns(['prompt', 'chosen', 'rejected'])\n",
        "formatted_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkwlGDCQ7wnB",
        "outputId": "84dcda8f-e8f9-4874-cc81-012a197420ee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['prompt', 'chosen', 'rejected'],\n",
              "        num_rows: 10287\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['prompt', 'chosen', 'rejected'],\n",
              "        num_rows: 2572\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "split_formatted_dataset = formatted_dataset[\"train\"].train_test_split(test_size=0.2)\n",
        "split_formatted_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xu4o5iL9Yihl"
      },
      "source": [
        "# Initialize the model\n",
        "\n",
        "Note that we initialize a **quantized** version of the model and we fine-tune **LoRa**. This is done to reduce memory consumption and allow for running this on consumer hardwarde and Google Colab. For a full fine-tune you would a lot more GPU resources.\n",
        "\n",
        "We have selected `microsoft/phi-2` as our main and reference model, so we will designate it in a variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lChdRaiR81Dc"
      },
      "outputs": [],
      "source": [
        "# Set our model\n",
        "model_name = \"microsoft/phi-2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkzPwEtpklF5"
      },
      "source": [
        "Then, we will load the tokenizer and configure padding. Remember to set `trust_remote_code=True`, so that it can be properly loaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Er231zIrkrGg",
        "outputId": "54f2f205-4187-4327-c39c-07d501a4225b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYz7F3q8kv-_"
      },
      "source": [
        "Next, we will load the **quantized model**, a crucial step that significantly enhances efficiency and performance. Quantization involves converting the model's weights and activations from floating-point to lower-precision formats. This process reduces the model's size, making it more memory-efficient and suitable for devices with limited storage but it comes at the cost of some accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "6b27fe48874945c989fb3750264b24a7",
            "bbca7e828ce24194929c177c602da9fd",
            "90ff72f28246473fbf92aed71dea1c94",
            "fcce17b68b0a40c7a6e7aeb8b3fbed53",
            "5907c82075b549f7b1ab3ac4a85dc068",
            "eb9005e4442849cc80e78fceae5915ea",
            "706fec4b9dba432997d65f3fe88e0fc8",
            "cb3d3a6b131d438593a610bc3cf2e2b6",
            "eb11a742f8c241b1a3e4f8b225e2d37a",
            "5513a18ce8c34629b9f318c0a43be64b",
            "c90f46a553204d35aa6db6807af1eb04"
          ]
        },
        "id": "D2mKNXOBk5YD",
        "outputId": "0383e2a9-f4f5-422f-e71b-20abecf15b99"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b27fe48874945c989fb3750264b24a7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "compute_dtype = getattr(torch, \"float16\")\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_compute_dtype='float16',\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, torch_dtype=torch.float16, quantization_config=bnb_config, trust_remote_code=True, device_map={\"\": 0}\n",
        ")\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "model.config.use_cache = False\n",
        "model.config.gradient_checkpointing = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz0LR5wMlLL2"
      },
      "source": [
        "Finally, we want to initialize the **LoRa** configuration. This will allow us to freeze the pre-trained model weights while dynamically adjusting only a small set of additional parameters. This approach reduces the computational burden and memory requirements, making it a more practical and resource-efficient way to customize pre-trained models. In this case, we will target the layers within the attention mechanism and the feed-forward networks, although you can choose to target other modules as identifying the best ones is still in progress.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnlPu6BYlI4d"
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.5,\n",
        "    r=32,\n",
        "    target_modules=['k_proj', 'q_proj', 'v_proj', 'fc1', 'fc2'],\n",
        "    bias=\"none\"`\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will also need a **reference model**, so we will initialize the `DPOTrainer` with `model_ref=None` so that you just have to load a single base model to compute both the reference and active logits by enabling / disabling the adapter.\n"
      ],
      "metadata": {
        "id": "r-Awvu22ymXu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNkr4H81lZxV"
      },
      "source": [
        "# Train the model\n",
        "\n",
        "Now, we will set the training arguments and start to fine-tune using the TRL [DPOTrainer](https://huggingface.co/docs/trl/main/en/dpo_trainer). Take into account that these parameters may differ depending on your exact purpose and hardware requirements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ng1lTIw-Ur4"
      },
      "outputs": [],
      "source": [
        "model_name = f\"phi2-lora-{dataset_name}\"\n",
        "os.environ[\"WANDB_PROJECT\"] = model_name  # name your W&B project\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"  # log all model checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnYP3pGynaXV",
        "outputId": "5991317d-d046-4fa3-f0a8-4bfe00e3f9e7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n"
          ]
        }
      ],
      "source": [
        "training_arguments = TrainingArguments(\n",
        "    output_dir=f\"./{model_name}\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    do_eval=True,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=16,\n",
        "    per_device_eval_batch_size=2,\n",
        "    log_level=\"debug\",\n",
        "    save_steps=250,\n",
        "    logging_steps=250,\n",
        "    learning_rate=1e-5,\n",
        "    eval_steps=250,\n",
        "    num_train_epochs=1, # Modified for tutorial purposes\n",
        "    warmup_steps=250,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    report_to=\"wandb\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXqoH2TMnjjp"
      },
      "outputs": [],
      "source": [
        " dpo_trainer = DPOTrainer(\n",
        "    model,\n",
        "    args=training_arguments,\n",
        "    beta=0.1,\n",
        "    peft_config=peft_config,\n",
        "    train_dataset=split_formatted_dataset[\"train\"],\n",
        "    eval_dataset=split_formatted_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    padding_value=tokenizer.pad_token_id,\n",
        ")\n",
        "\n",
        "dpo_trainer.train()\n",
        "dpo_trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VKAmb7u7V0t"
      },
      "outputs": [],
      "source": [
        "dpo_trainer.push_to_hub(f\"argilla/{model_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDF0GLFwnxDZ"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "In this tutorial, we have explored a method to fine-tune large language models using a pool of annotators. In particular, we have used Prolific to gather responses from diverse group of annotators. We then analyzed the responses using argilla. Finally, we have fine-tuned microsoft/phi-2 using DPO, quantization and LoRa.\n",
        "\n",
        "Even though this tutorial is focused on a specific LM, the approach outlined can be adapted to other models and tasks. In addition, To further boost performance, consider experimenting with a range of parameters. We encourage you to explore the different options"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxeKXBZ-uZ3D"
      },
      "source": [
        "# Next steps\n",
        "\n",
        "## Intersting resources\n",
        "\n",
        "- [Ollama](https://ollama.ai/) to Get up and running with large language models, locally. Don't forget to check our [notus blog](https://argilla.io/blog/notus7b/) and [model](https://ollama.ai/argilla/notus) on ollama.\n",
        "- [TRL](https://github.com/lvwerra/trl) is a full stack library where we provide a set of tools to train transformer language models.\n",
        "- [bits and bytes](https://www.google.com/search?client=firefox-b-d&q=eli5+bits+and+bytes) allow users to run models in 4-bit precision.\n",
        "- [LoRa](https://www.reddit.com/r/MachineLearning/comments/13m78u6/d_an_eli5_explanation_for_lora_lowrank_adaptation/) reduces the computational burden and memory requirements by fine-tuning a small set of additional parameters.\n",
        "- [TheBloke](https://huggingface.co/TheBloke) for wonderful LLM quantisation and fine tuning.\n",
        "\n",
        "## Shameless self-promoting\n",
        "\n",
        "### Personal\n",
        "\n",
        "- [LinkedIn](https://www.linkedin.com/in/david-berenstein-1bab11105/)\n",
        "- [Twitter](https://twitter.com/davidbstein1957)\n",
        "- [GitHub](https://github.com/davidberenstein1957)\n",
        "\n",
        "### Company\n",
        "\n",
        "- [Argilla Github](https://github.com/argilla-io/argilla)\n",
        "- [Distilabel Github](https://github.com/argilla-io/distilabel)\n",
        "- [Argilla Slack Community](https://join.slack.com/t/rubrixworkspace/shared_invite/zt-whigkyjn-a3IUJLD7gDbTZ0rKlvcJ5g)\n",
        "- [Bi-weekly NLP community meetup](https://lu.ma/d720wy9f)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "conda_pytorch_p310",
      "language": "python",
      "name": "conda_pytorch_p310"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5513a18ce8c34629b9f318c0a43be64b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "2.0.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5907c82075b549f7b1ab3ac4a85dc068": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "2.0.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b27fe48874945c989fb3750264b24a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bbca7e828ce24194929c177c602da9fd",
              "IPY_MODEL_90ff72f28246473fbf92aed71dea1c94",
              "IPY_MODEL_fcce17b68b0a40c7a6e7aeb8b3fbed53"
            ],
            "layout": "IPY_MODEL_5907c82075b549f7b1ab3ac4a85dc068",
            "tabbable": null,
            "tooltip": null
          }
        },
        "706fec4b9dba432997d65f3fe88e0fc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "HTMLStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "90ff72f28246473fbf92aed71dea1c94": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_cb3d3a6b131d438593a610bc3cf2e2b6",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb11a742f8c241b1a3e4f8b225e2d37a",
            "tabbable": null,
            "tooltip": null,
            "value": 2
          }
        },
        "bbca7e828ce24194929c177c602da9fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_eb9005e4442849cc80e78fceae5915ea",
            "placeholder": "​",
            "style": "IPY_MODEL_706fec4b9dba432997d65f3fe88e0fc8",
            "tabbable": null,
            "tooltip": null,
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c90f46a553204d35aa6db6807af1eb04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "HTMLStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "cb3d3a6b131d438593a610bc3cf2e2b6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "2.0.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb11a742f8c241b1a3e4f8b225e2d37a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eb9005e4442849cc80e78fceae5915ea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "2.0.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcce17b68b0a40c7a6e7aeb8b3fbed53": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_5513a18ce8c34629b9f318c0a43be64b",
            "placeholder": "​",
            "style": "IPY_MODEL_c90f46a553204d35aa6db6807af1eb04",
            "tabbable": null,
            "tooltip": null,
            "value": " 2/2 [00:02&lt;00:00,  1.04s/it]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}