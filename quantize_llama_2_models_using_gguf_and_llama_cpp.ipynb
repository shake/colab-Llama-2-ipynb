{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shake/colab-Llama-2-ipynb/blob/main/quantize_llama_2_models_using_gguf_and_llama_cpp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantize Llama 2 models using GGUF and llama.cpp\n",
        "> üó£Ô∏è [Large Language Model Course](https://github.com/mlabonne/llm-course)\n",
        "\n",
        "‚ù§Ô∏è Created by [@maximelabonne](https://twitter.com/maximelabonne).\n",
        "\n",
        "## Usage\n",
        "\n",
        "* `MODEL_ID`: The ID of the model to quantize (e.g., `mlabonne/EvolCodeLlama-7b`).\n",
        "* `QUANTIZATION_METHOD`: The quantization method to use.\n",
        "\n",
        "## Quantization methods\n",
        "\n",
        "The names of the quantization methods follow the naming convention: \"q\" + the number of bits + the variant used (detailed below). Here is a list of all the possible quant methods and their corresponding use cases, based on model cards made by [TheBloke](https://huggingface.co/TheBloke/):\n",
        "\n",
        "* `q2_k`: Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for the other tensors.\n",
        "* `q3_k_l`: Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\n",
        "* `q3_k_m`: Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\n",
        "* `q3_k_s`: Uses Q3_K for all tensors\n",
        "* `q4_0`: Original quant method, 4-bit.\n",
        "* `q4_1`: Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models.\n",
        "* `q4_k_m`: Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K\n",
        "* `q4_k_s`: Uses Q4_K for all tensors\n",
        "* `q5_0`: Higher accuracy, higher resource usage and slower inference.\n",
        "* `q5_1`: Even higher accuracy, resource usage and slower inference.\n",
        "* `q5_k_m`: Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K\n",
        "* `q5_k_s`:  Uses Q5_K for all tensors\n",
        "* `q6_k`: Uses Q8_K for all tensors\n",
        "* `q8_0`: Almost indistinguishable from float16. High resource use and slow. Not recommended for most users.\n",
        "\n",
        "As a rule of thumb, **I recommend using Q5_K_M** as it preserves most of the model's performance. Alternatively, you can use Q4_K_M if you want to save some memory. In general, K_M versions are better than K_S versions. I cannot recommend Q2_K or Q3_* versions, as they drastically decrease model performance."
      ],
      "metadata": {
        "id": "8y_Rk94LzG7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ÂÆâË£ÖÈúÄË¶Å‰æùËµñ\n",
        "%%capture\n",
        "!pip install -q huggingface_hub"
      ],
      "metadata": {
        "id": "de_HraNsM5Mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import ÂØÜÈí•Ôºåtoken\n",
        "%%capture\n",
        "from google.colab import userdata\n",
        "hf_token = userdata.get('huggingface')\n",
        "!git config --global credential.helper store\n",
        "!huggingface-cli login --token $hf_token --add-to-git-credential"
      ],
      "metadata": {
        "id": "veee0yaMM4dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables\n",
        "MODEL_ID = \"chenshake/Llama-2-7b-hf\"\n",
        "QUANTIZATION_METHODS = [\"q4_k_m\", \"q5_k_m\"]\n",
        "\n",
        "# Constants\n",
        "MODEL_NAME = MODEL_ID.split('/')[-1]\n"
      ],
      "metadata": {
        "id": "fD24jJxq7t3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install llama.cpp\n",
        "%%capture\n",
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "!cd llama.cpp && git pull && make clean && LLAMA_CUBLAS=1 make\n",
        "!pip install -r llama.cpp/requirements.txt\n"
      ],
      "metadata": {
        "id": "FHrUsJmCMYgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download model\n",
        "%%capture\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/{MODEL_ID}"
      ],
      "metadata": {
        "id": "0RCrmbY3MVdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to fp16\n",
        "%%capture\n",
        "fp16 = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.fp16.bin\"\n",
        "!python llama.cpp/convert.py {MODEL_NAME} --outtype f16 --outfile {fp16}"
      ],
      "metadata": {
        "id": "DZfyQm-ZMSbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantize the model for each method in the QUANTIZATION_METHODS list.  colab‰∏äÈúÄË¶ÅÈÄâÊã©T4ÔºåÂêØÂä®cudaÔºåÂê¶Âàô‰ºöÊä•Èîô„ÄÇ\n",
        "for method in QUANTIZATION_METHODS:\n",
        "    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\"\n",
        "    !./llama.cpp/quantize {fp16} {qtype} {method}"
      ],
      "metadata": {
        "id": "Og3X-mgvMP7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Push to hub\n",
        "\n",
        "To push your model to the hub, you'll need to input your Hugging Face token (https://huggingface.co/settings/tokens) in Google Colab's \"Secrets\" tab. The following code creates a new repo with the \"-GGUF\" suffix. Don't forget to change the `username` variable."
      ],
      "metadata": {
        "id": "Ar8pO7bb80US"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import huggingface_hub\n",
        "username = f'{huggingface_hub.whoami()[\"name\"]}'\n",
        "print(username)"
      ],
      "metadata": {
        "id": "UPqgxn6IXn9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ë∞ÉÁî®apiÔºåÂú®‰Ω†ÁöÑhugingfaceË¥¶Âè∑‰∏ãÔºåÂàõÂª∫repoÔºåÂ≠òÊîæÊ®°ÂûãÔºåËÆ∞Âæó‰øÆÊîπ‰∏äÈù¢ÁöÑÁî®Êà∑ÂêçÁöÑÂèòÈáè„ÄÇ\n",
        "from huggingface_hub import create_repo, HfApi\n",
        "\n",
        "# Create empty repo\n",
        "create_repo(\n",
        "    repo_id=f\"{username}/{MODEL_NAME}-GGUF\",\n",
        "    repo_type=\"model\",\n",
        "    exist_ok=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "hR-ZodY6X6md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload gguf files\n",
        "# Defined in the secrets tab in Google Colab\n",
        "api = HfApi(token=userdata.get(\"huggingface\"))\n",
        "api.upload_folder(\n",
        "    folder_path=MODEL_NAME,\n",
        "    repo_id=f\"{username}/{MODEL_NAME}-GGUF\",\n",
        "    allow_patterns=f\"*.gguf\",\n",
        ")"
      ],
      "metadata": {
        "id": "X995beyuYNiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tenGZ01TYGvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# backup"
      ],
      "metadata": {
        "id": "twjtUK2nZ-TP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run inference\n",
        "\n",
        "Here is a simple script to run your quantized models. I'm offloading every layer to the GPU (35 for a 7b parameter model) to speed up inference."
      ],
      "metadata": {
        "id": "WqI1CPiXI4dP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "model_list = [file for file in os.listdir(MODEL_NAME) if \"gguf\" in file]\n",
        "\n",
        "prompt = input(\"Enter your prompt: \")\n",
        "chosen_method = input(\"Name of the model (options: \" + \", \".join(model_list) + \"): \")\n",
        "\n",
        "# Verify the chosen method is in the list\n",
        "if chosen_method not in model_list:\n",
        "    print(\"Invalid name\")\n",
        "else:\n",
        "    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\"\n",
        "    !./llama.cpp/main -m {qtype} -n 128 --color -ngl 35 -p \"{prompt}\""
      ],
      "metadata": {
        "id": "vNPL9WYg78l-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}